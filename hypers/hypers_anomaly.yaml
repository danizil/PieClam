#    db    8b    d8    db    8888P  dP"Yb  88b 88 
#   dPYb   88b  d88   dPYb     dP  dP   Yb 88Yb88 
#  dP__Yb  88YbdP88  dP__Yb   dP   Yb   dP 88 Y88 
# dP""""Yb 88 YY 88 dP""""Yb d8888  YbodP  88  Y8 

amazonGGAD_bigclam:
    clamiter_init:
        dim_feat: 18
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1

amazonGGAD_iegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1

amazonGGAD_pclam:
    clamiter_init:
        dim_feat: 18
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    back_forth:
        n_back_forth: 1
        scheduler_step_size: 600
        scheduler_gamma: 0.5

amazonGGAD_piegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    back_forth:
        n_back_forth: 0
        scheduler_step_size: 600
        scheduler_gamma: 0.5

# 88""Yb 888888 8888b.  8888b.  88 888888 
# 88__dP 88__    8I  Yb  8I  Yb 88   88   
# 88"Yb  88""    8I  dY  8I  dY 88   88   
# 88  Yb 888888 8888Y"  8888Y"  88   88   

redditGGAD_bigclam:
    clamiter_init:
        dim_feat: 18
        dim_attr: 64
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 1700

redditGGAD_iegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 40
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 2000
        

redditGGAD_pclam:
    clamiter_init:
        dim_feat: 18
        dim_attr: 40
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    prior_opt:
        n_iter: 10000
        lr: 0.0001
        noise_amp: 0.005
        weight_decay: 0.1
        
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_prior'

redditGGAD_piegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 64
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    prior_opt:
        n_iter: 10000
        lr: 0.0001
        noise_amp: 0.005
        weight_decay: 0.1
        
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_prior'

        


# 888888 88     88     88 88""Yb 888888 88  dP""b8 
# 88__   88     88     88 88__dP   88   88 dP   `" 
# 88""   88  .o 88  .o 88 88"""    88   88 Yb      
# 888888 88ood8 88ood8 88 88       88   88  YboodP 

ellipticGGAD_bigclam:
    clamiter_init:
        dim_feat: 18
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 20000
        

ellipticGGAD_iegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 20000
       

ellipticGGAD_pclam:
    clamiter_init:
        dim_feat: 10
        dim_attr: 70
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    feat_opt:
        lr: 0.00001
        n_iter: 100
       
    prior_opt:
        lr: 0.00001
        n_iter: 1000
        noise_amp: 0.05
        weight_decay: 0.1
       
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_prior'
       

ellipticGGAD_piegam:
# the configuration is the same as the global
        


# 88""Yb 88      dP"Yb   dP""b8 
# 88__dP 88     dP   Yb dP   `" 
# 88""Yb 88  .o Yb   dP Yb  "88 
# 88oodP 88ood8  YbodP   YboodP 

BlogCatalog_bigclam:
    clamiter_init:
        dim_feat: 18
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 1700

BlogCatalog_iegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 2000

BlogCatalog_pclam:
    clamiter_init:
        dim_feat: 18
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 32
        num_coupling_blocks: 32 
        num_layers_mlp: 4
    feat_opt:
        lr: 0.000001
        n_iter: 1000
    prior_opt:
        n_iter: 400000
        lr: 0.000001
        noise_amp: 0.1
        weight_decay: 0.1
       
    back_forth:
        n_back_forth: 1
        scheduler_step_size: 600
        scheduler_gamma: 0.5

BlogCatalog_piegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 32
        num_coupling_blocks: 32 
        num_layers_mlp: 4
    feat_opt:
        lr: 0.000001
        n_iter: 1000
    prior_opt:
        n_iter: 20000
        lr: 0.000001
        noise_amp: 0.001
        weight_decay: 0.1
       

# 888888 88     88  dP""b8 88  dP 88""Yb 
# 88__   88     88 dP   `" 88odP  88__dP 
# 88""   88  .o 88 Yb      88"Yb  88"Yb  
# 88     88ood8 88  YboodP 88  Yb 88  Yb 

Flickr_bigclam:
    clamiter_init:
        dim_feat: 18
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 17000
       

Flickr_iegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 20000
       

Flickr_pclam:
    clamiter_init:
        dim_feat: 20
        dim_attr: 40
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    feat_opt:
        lr: 0.00001
        n_iter: 1000
       
    prior_opt:
        lr: 0.00001
        n_iter: 1000
        noise_amp: 0.0.05
        weight_decay: 0.1
       
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_prior'
       

Flickr_piegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 40
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    feat_opt:
        lr: 0.000001
        n_iter: 1000
       
    prior_opt:
        lr: 0.0001
        n_iter: 10000
        noise_amp: 0.005
        weight_decay: 0.1
       
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_prior'
       

#    db     dP""b8 8b    d8 
#   dPYb   dP   `" 88b  d88 
#  dP__Yb  Yb      88YbdP88 
# dP""""Yb  YboodP 88 YY 88 

ACM_bigclam:
    clamiter_init:
        dim_feat: 18
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 1700

ACM_iegam:
    clamiter_init:
        dim_feat: 24
        dim_attr: 32
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 2000

ACM_pclam:
    clamiter_init:
        dim_feat: 25
        dim_attr: 200
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    feat_opt:
        lr: 0.000001
        n_iter: 500
       
    prior_opt:
        lr: 0.00001
        n_iter: 1000
        noise_amp: 0.05
        weight_decay: 0.1
       
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_prior'
       

ACM_piegam:
    clamiter_init:
        dim_feat: 34
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 32
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    feat_opt:
        lr: 0.000001
        n_iter: 500
       
    prior_opt:
        lr: 0.00001
        n_iter: 1000
        noise_amp: 0.01
        weight_decay: 0.1
       
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_prior'
       


# 88""Yb 88  88  dP"Yb  888888  dP"Yb  
# 88__dP 88  88 dP   Yb   88   dP   Yb 
# 88"""  888888 Yb   dP   88   Yb   dP 
# 88     88  88  YbodP    88    YbodP  

photoGGAD_bigclam:
    clamiter_init:
        dim_feat: 30
        dim_attr: 500
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 20000
        

photoGGAD_iegam:
    clamiter_init:
        dim_feat: 30
        dim_attr: 500
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 20000
        

photoGGAD_pclam:
    clamiter_init:
        dim_feat: 20
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    feat_opt:
        lr: 0.00001
        n_iter: 100
        
    prior_opt:
        lr: 0.00001
        n_iter: 1000
        noise_amp: 0.05
        weight_decay: 0.1
        
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_prior'
        

photoGGAD_piegam:
    clamiter_init:
       s_reg : 0.0
    feat_opt:
        lr: 0.00001
        n_iter: 500
    prior_opt:
        lr: 0.00001
        n_iter: 1000
        noise_amp: 0.05
        weight_decay: 0.1
        
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_prior'


# interesting values:
# 0.01, 32, 32, 2: give more iterations in the second classify anomaly. seems to not saturate. seems that way also after many tries! not so sure that that is true. for 0.5 i think that 3 layer mlp might have been better
# what happens when the loss climbs really high? is it good for anomaly detection? 
# these numbers are the first time i went over the base in 0.2! and maybe can still be made better!


#  dP""b8 88      dP"Yb  88""Yb    db    88     
# dP   `" 88     dP   Yb 88__dP   dPYb   88     
# Yb  "88 88  .o Yb   dP 88""Yb  dP__Yb  88  .o 
#  YboodP 88ood8  YbodP  88oodP dP""""Yb 88ood8 


MightyConfigs_iegam:
    clamiter_init: 
        dim_feat: 30
        dim_attr: 64
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32
        num_layers_mlp: 2
    feat_opt: 
        lr: 0.000001
        n_iter: 2000
        early_stop: 0
    

MightyConfigs_piegam:
    clamiter_init: 
        dim_feat: 30
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32
        num_layers_mlp: 2
    feat_opt: 
        lr: 0.000003
        n_iter: 500
        early_stop: 0
    prior_opt:
        n_iter: 1300
        lr: 0.000002
        noise_amp: 0.05
        weight_decay: 0.1
        early_stop: 0
    back_forth:
        n_back_forth: 3
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        early_stop_fit: 0
        first_func_in_fit: "fit_feats"


MightyConfigs_bigclam:
    clamiter_init: 
        dim_feat: 24
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32
        num_layers_mlp: 2
    feat_opt: 
        lr: 0.000001
        n_iter: 200
        early_stop: 0


MightyConfigs_pclam:
    clamiter_init: 
        dim_feat: 30
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32
        num_layers_mlp: 2
    feat_opt: 
        lr: 0.000003
        n_iter: 500
        early_stop: 0
    prior_opt:
        n_iter: 1300
        lr: 0.0000002
        noise_amp: 0.05
        weight_decay: 0.1
        early_stop: 0
    back_forth:
        n_back_forth: 3
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        early_stop_fit: 0
        first_func_in_fit: "fit_feats"
